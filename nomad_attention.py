# -*- coding: utf-8 -*-
"""NoMAD-attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xua-6zLprDv3sJo7n0ReKRNtJCEjC0qv
"""

!pip install transformers torch datasets accelerate bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch
import numpy as np

model_name = "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu", torch_dtype=torch.float16)

test_data = load_dataset("wikitext", "wikitext-2-v1", split="test")
test_texts = [text for text in test_data["text"] if len(text) > 0]

def compute_perplexity(model, tokenizer, texts, max_length=512):
    nlls = []
    for text in texts[:20]:  # Use first 20 samples for speed
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            nlls.append(outputs.loss.item())
    return np.exp(np.mean(nlls))

orig_ppl = compute_perplexity(model, tokenizer, test_texts)
print(f"Original Perplexity: {orig_ppl:.2f}")

"""Mock NoMAD-Attention"""

import numpy as np
from sklearn.cluster import KMeans


class NoMADAttentionWrapper:
    def __init__(self, model, n_centroids=16, d_sub=64):
        self.model = model
        self.n_centroids = n_centroids
        self.d_sub = d_sub
        self.centroids = None

    def train_centroids(self, calibration_texts):
        # Extract key vectors
        key_vectors = []
        for text in calibration_texts[:50]:  # Small calibration set
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                outputs = self.model(**inputs, output_hidden_states=True)
                key_vectors.append(outputs.hidden_states[-1].mean(dim=1).numpy())  # [1, d_model]
        key_vectors = np.vstack(key_vectors)  # [n_samples, d_model]

        # Train centroids per sub-quantizer
        S = key_vectors.shape[1] // self.d_sub
        self.centroids = []
        for s in range(S):
            sub_vecs = key_vectors[:, s*self.d_sub : (s+1)*self.d_sub]
            kmeans = KMeans(n_clusters=self.n_centroids).fit(sub_vecs)
            self.centroids.append(kmeans.cluster_centers_)

    def nomad_forward(self, input_ids):
        # Mock: Replace dot-products with lookup-based approximation
        inputs = tokenizer(input_ids, return_tensors="pt", return_attention_mask=False)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.logits

nomad_model = NoMADAttentionWrapper(model)
nomad_model.train_centroids(test_texts[:50])

def compute_noMAD_perplexity(nomad_model, tokenizer, texts):
    nlls = []
    for text in texts[:20]:  # Same samples as baseline
        logits = nomad_model.nomad_forward(text)
        # Mock loss calculation (real impl. requires full attention replacement)
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            loss = torch.nn.functional.cross_entropy(
                logits[:, :-1].reshape(-1, logits.shape[-1]),
                inputs["input_ids"][:, 1:].reshape(-1))
            nlls.append(loss.item())
    return np.exp(np.mean(nlls))

nomad_ppl = compute_noMAD_perplexity(nomad_model, tokenizer, test_texts)
print(f"NoMAD Perplexity: {nomad_ppl:.2f}")

print(f"Original PPL: {orig_ppl:.2f} | NoMAD PPL: {nomad_ppl:.2f}")
print(f"Relative Change: {((nomad_ppl - orig_ppl) / orig_ppl * 100):.1f}%")

"""inference time"""

import time

def benchmark_inference(model, tokenizer, texts, method="original"):
    times = []
    for text in texts[:20]:  # Same 20 samples
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        start = time.time()
        if method == "original":
            with torch.no_grad():
                _ = model(**inputs)
        else:  # NoMAD
            _ = nomad_model.nomad_forward(text)
        times.append(time.time() - start)
    return np.mean(times) * 1000

# Benchmark
orig_time = benchmark_inference(model, tokenizer, test_texts, "original")
nomad_time = benchmark_inference(nomad_model, tokenizer, test_texts, "nomad")

print(f"Original Attention Time: {orig_time:.1f}ms")
print(f"NoMAD Time: {nomad_time:.1f}ms")
print(f"Speedup: {orig_time / nomad_time:.1f}x")